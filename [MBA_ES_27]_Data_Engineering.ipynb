{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielgsv/ClinicaBack/blob/master/%5BMBA_ES_27%5D_Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbw_w-qFariJ",
        "outputId": "a7e02f47-b831-4df8-fc38-cc42e745bd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dos datasets e arquivos de exemplos\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/cars.json\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/cidades.json\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/dados+cota%C3%A7%C3%B5es/BBDC4_10y.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/Players.csv\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/czech-financial-dataset-real-anonymized-transactions.zip\n",
        "!wget https://raw.githubusercontent.com/spark-examples/pyspark-examples/master/resources/zipcodes.json\n",
        "!unzip czech-financial-dataset-real-anonymized-transactions.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQyHz4rnbFnq",
        "outputId": "36e8f3c4-78e3-497f-b980-058c2f8bc085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-28 23:18:28--  https://datasets-aulas.s3.amazonaws.com/cars.json\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1580 (1.5K) [application/json]\n",
            "Saving to: ‚Äòcars.json‚Äô\n",
            "\n",
            "cars.json           100%[===================>]   1.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:29 (42.8 MB/s) - ‚Äòcars.json‚Äô saved [1580/1580]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://datasets-aulas.s3.amazonaws.com/cidades.json\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6925 (6.8K) [application/json]\n",
            "Saving to: ‚Äòcidades.json‚Äô\n",
            "\n",
            "cidades.json        100%[===================>]   6.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:29 (156 MB/s) - ‚Äòcidades.json‚Äô saved [6925/6925]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://datasets-aulas.s3.amazonaws.com/dados+cota%C3%A7%C3%B5es/BBDC4_10y.csv\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299951 (293K) [text/csv]\n",
            "Saving to: ‚ÄòBBDC4_10y.csv‚Äô\n",
            "\n",
            "BBDC4_10y.csv       100%[===================>] 292.92K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-10-28 23:18:29 (2.51 MB/s) - ‚ÄòBBDC4_10y.csv‚Äô saved [299951/299951]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‚Äòadult.data‚Äô\n",
            "\n",
            "adult.data              [      <=>           ]   3.79M  3.63MB/s    in 1.0s    \n",
            "\n",
            "2024-10-28 23:18:31 (3.63 MB/s) - ‚Äòadult.data‚Äô saved [3974305]\n",
            "\n",
            "--2024-10-28 23:18:31--  https://datasets-aulas.s3.amazonaws.com/Players.csv\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.33.220, 52.216.49.113, 54.231.228.105, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.33.220|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 281111 (275K) [text/csv]\n",
            "Saving to: ‚ÄòPlayers.csv‚Äô\n",
            "\n",
            "Players.csv         100%[===================>] 274.52K   499KB/s    in 0.6s    \n",
            "\n",
            "2024-10-28 23:18:32 (499 KB/s) - ‚ÄòPlayers.csv‚Äô saved [281111/281111]\n",
            "\n",
            "--2024-10-28 23:18:32--  https://datasets-aulas.s3.amazonaws.com/czech-financial-dataset-real-anonymized-transactions.zip\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.33.220, 52.216.49.113, 54.231.228.105, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.33.220|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35693438 (34M) [application/zip]\n",
            "Saving to: ‚Äòczech-financial-dataset-real-anonymized-transactions.zip‚Äô\n",
            "\n",
            "czech-financial-dat 100%[===================>]  34.04M  38.6MB/s    in 0.9s    \n",
            "\n",
            "2024-10-28 23:18:33 (38.6 MB/s) - ‚Äòczech-financial-dataset-real-anonymized-transactions.zip‚Äô saved [35693438/35693438]\n",
            "\n",
            "--2024-10-28 23:18:33--  https://raw.githubusercontent.com/spark-examples/pyspark-examples/master/resources/zipcodes.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7331 (7.2K) [text/plain]\n",
            "Saving to: ‚Äòzipcodes.json‚Äô\n",
            "\n",
            "zipcodes.json       100%[===================>]   7.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:33 (51.2 MB/s) - ‚Äòzipcodes.json‚Äô saved [7331/7331]\n",
            "\n",
            "Archive:  czech-financial-dataset-real-anonymized-transactions.zip\n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/datapackage.json  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/account.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/card.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/client.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/disp.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/district.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/loan.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/order.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/trans.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/account.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/card.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/client.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/data map.gif  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/disp.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/district.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/loan.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/order.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/trans.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "# üë®‚Äçüî¨ Testando a instala√ß√£o\n",
        "Agora, podemos importar **```SparkSession```** de **```pyspark.sql```** e criar uma **```SparkSession```**. **```SparkSession```** foi introduzido na vers√£o **Spark 2.0** e √© um ponto de entrada para a funcionalidade Spark subjacente para criar programaticamente **RDDs**, **DataFrames** e **DataSets**.\n",
        "\n",
        "Voc√™ pode dar um nome √† sess√£o usando **```appName()```** e adicionando algumas configura√ß√µes com **```config()```** se desejar."
      ],
      "metadata": {
        "id": "vZNT_lgfcL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "qP5P5qpucMo3",
        "outputId": "460c37bd-e0a1-4478-d036-120c30d610a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f41cde65fc0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b5571be3fd6e:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSession\n",
        "Desde o Spark 2.0, o **```SparkSession```** se tornou um ponto de entrada para o PySpark trabalhar com **```RDD```** e **```DataFrame```**. Antes da vers√£o 2.0, o **```SparkContext```** costumava ser um ponto de entrada. O **```SparkSession```** √© uma classe combinada para todos os diferentes contextos que t√≠nhamos antes do lan√ßamento 2.0 ( **```SparkContext```**, **```StreamingContext```**, **```SQLContext```**, **```HiveContext```**, etc).\n",
        "\n",
        "Para criar **```SparkSession```** programaticamente no PySpark, voc√™ precisa usar o m√©todo padr√£o do construtor **```builder()```**. O m√©todo **```getOrCreate()```** retorna um **```SparkSession```** j√° existente.; se n√£o existir, cria uma nova **```SparkSession```**.\n",
        "\n",
        "**```master()```** ‚Äì Se voc√™ estiver executando no cluster, voc√™ precisar√° usar seu nome mestre como argumento para **```master()```**. Normalmente, ser√° **```yarn```** ou **```mesos```**, dependendo da configura√ß√£o do seu cluster. Use **```local[x]```** ao executar no modo **Standalone** onde **```x```** deve ser um valor inteiro e deve ser maior que **```0```**. Isso representa quantas parti√ß√µes ele deve criar ao usar um **```RDD```**, **```DataFrame```** ou **```Dataset```**. Idealmente, o valor **```x```** deve ser o n√∫mero de n√∫cleos de CPU que voc√™ possui.\n",
        "\n",
        "**```appName()```** ‚Äì Usado para definir o nome do aplicativo.\n",
        "\n",
        "**```getOrCreate()```** ‚Äì Retorna um objeto **```SparkSession```** se j√° existir e cria um novo se n√£o existir.\n",
        "\n",
        "\\\n",
        "## Usando **```SparkConfig```**\n",
        "Se voc√™ quiser definir algumas configura√ß√µes para **```SparkSession```**, use o m√©todo **```config()```**.\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[1]\") \\\n",
        "      .appName(\"Exemplo\") \\\n",
        "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
        "      .getOrCreate()\n",
        "```"
      ],
      "metadata": {
        "id": "MJFRPX-6dIlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primeiro exemplo: Total de pa√≠ses distintos presentes no dataset\n",
        "üá®üá∫ üáÆüá≥ üá≤üáΩ üáøüá¶ üáµüá∑ üá≠üá≥ üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø üá®üá¶ üá©üá™ üáÆüá∑ üáÆüáπ üáµüá± üá®üá¥ üá∞üá≠ üáπüá≠ üá±üá¶ üá≠üáπ üá¨üáπ üáµüá™ üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø üáπüáπ üá¨üá∑ üá≥üáÆ üá≥üá± üá∫üá∏ üáØüá≤ üáµüá≠ üá™üá® üáπüáº üáµüáπ üá©üá¥ üá∏üáª üá´üá∑ üá®üá≥ üáØüáµ üá≠üá∑üá∏üáÆüá≤üá∞üáßüá¶üá≤üá™ üá¨üá∫ üáªüá≥ üá≠üá∞ üáÆüá™ üá≠üá∫"
      ],
      "metadata": {
        "id": "wP7sl-CMdjKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configura√ß√£o do Spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Total de pa√≠ses distintos presentes no dataset\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Carrega o conjunto de dados a partir do arquivo CSV\n",
        "adultDatasetRDD = spark.sparkContext.textFile(\"adult.data\")\n",
        "\n",
        "# Divide cada linha do conjunto de dados por v√≠rgula, removendo linhas vazias\n",
        "splitAdultDatasetRDD = adultDatasetRDD.filter(lambda linha: linha != '').map(lambda linha: linha.split(\",\"))\n",
        "\n",
        "# Seleciona a coluna correspondente ao pa√≠s (√≠ndice 13) e obt√©m valores distintos\n",
        "paises = splitAdultDatasetRDD.map(lambda linha: linha[13]).distinct()\n",
        "\n",
        "# Conta a quantidade de pa√≠ses distintos\n",
        "count_paises = paises.count()\n",
        "\n",
        "# Coleta os pa√≠ses distintos como uma lista\n",
        "paises_list = paises.collect()\n",
        "\n",
        "# Imprime o resultado\n",
        "print(f\"Quantidade de pa√≠ses distintos: {count_paises}\")\n",
        "print(\"Pa√≠ses distintos:\", paises_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIkgPwqcdjzz",
        "outputId": "d5634e2b-ddae-4a94-b960-7ed00c6c56de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de pa√≠ses distintos: 42\n",
            "Pa√≠ses distintos: [' Cuba', ' India', ' Mexico', ' South', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Italy', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Laos', ' Haiti', ' Guatemala', ' Peru', ' Scotland', ' Trinadad&Tobago', ' Greece', ' Nicaragua', ' Holand-Netherlands', ' United-States', ' Jamaica', ' ?', ' Philippines', ' Ecuador', ' Taiwan', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' China', ' Japan', ' Yugoslavia', ' Outlying-US(Guam-USVI-etc)', ' Vietnam', ' Hong', ' Ireland', ' Hungary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "# Dataframes\n",
        "O Apache Spark **DataFrame** √© uma estrutura de dados tabular distribu√≠da, oferecendo uma interface de alto n√≠vel para manipula√ß√£o eficiente de grandes conjuntos de dados. Inspirado pela simplicidade das tabelas em bancos de dados relacionais, o DataFrame do Spark simplifica opera√ß√µes complexas, permitindo an√°lises poderosas em ambientes distribu√≠dos.\n",
        "\n",
        "Sua abstra√ß√£o intuitiva facilita a execu√ß√£o de transforma√ß√µes e consultas SQL, proporcionando flexibilidade e desempenho otimizado para processamento de dados em escala. Com suporte a diversas fontes de dados e integra√ß√£o perfeita com o ecossistema Spark, o DataFrame se tornou uma ferramenta fundamental para cientistas de dados e engenheiros de dados na constru√ß√£o de pipelines de an√°lise robustos e eficientes.\n",
        "\n",
        "Defini√ß√£o de um [DataFrame](https://www.databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html) pela Databricks:\n",
        "> DataFrame √© uma cole√ß√£o distribu√≠da de dados organizados em colunas nomeadas. √â conceitualmente equivalente a uma tabela em um banco de dados relacional ou a um quadro de dados em R/Python, mas com otimiza√ß√µes mais ricas nos bastidores. DataFrames podem ser constru√≠dos a partir de uma ampla variedade de fontes, como arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes. ‚Äì Databricks"
      ],
      "metadata": {
        "id": "lAbcSJX1vEEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "## Criando um DataFrame\n",
        "A maneira mais simples de criar um DataFrame √© a partir de uma lista de dados Python. O DataFrame tamb√©m pode ser criado a partir de um RDD e lendo arquivos de diversas fontes."
      ],
      "metadata": {
        "id": "MWrf0t0-vH_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados = [('Jo√£o','','Silva','1991-04-01','M',3000),\n",
        "  ('Miguel','Ribeiro','','2000-05-19','M',4000),\n",
        "  ('Roberto','','Ferreira','1978-09-05','M',4000),\n",
        "  ('Maria','Ana','Oliveira','1967-12-01','F',4000),\n",
        "  ('J√©ssica','Mariana','Brito','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"primeiro_nome\",\"nome_meio\",\"sobrenome\",\"data_nascimento\",\"genero\",\"salario\"]\n",
        "df = spark.createDataFrame(data=dados, schema = columns)"
      ],
      "metadata": {
        "id": "c3DpX2YMvMUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8nKO9UwvdLN",
        "outputId": "6f024b40-cec8-4b8c-bbbe-8c09b3b21aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+---------+---------------+------+-------+\n",
            "|primeiro_nome|nome_meio|sobrenome|data_nascimento|genero|salario|\n",
            "+-------------+---------+---------+---------------+------+-------+\n",
            "|         Jo√£o|         |    Silva|     1991-04-01|     M|   3000|\n",
            "|       Miguel|  Ribeiro|         |     2000-05-19|     M|   4000|\n",
            "|      Roberto|         | Ferreira|     1978-09-05|     M|   4000|\n",
            "|        Maria|      Ana| Oliveira|     1967-12-01|     F|   4000|\n",
            "|      J√©ssica|  Mariana|    Brito|     1980-02-17|     F|     -1|\n",
            "+-------------+---------+---------+---------------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "## Lendo um arquivo CSV no DataFrame\n",
        "PySpark fornece **```csv(\"path\")```** no DataFrameReader para ler um arquivo CSV no PySpark DataFrame e **```dataframeObj.write.csv(\"path\")```** para salvar ou gravar no arquivo CSV.\n",
        "\n",
        "Este exemplo l√™ os dados nas colunas DataFrame **```_c0```** para a primeira coluna e **```_c1```** para a segunda e assim por diante. e por padr√£o o tipo de dados para todas essas colunas √© tratado como String."
      ],
      "metadata": {
        "id": "B04wedpZvsFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Lendo um arquivo CSV no DataFrame\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wXhZ9YxvvcA",
        "outputId": "c9b24732-8b1d-41d9-d40a-6555e398b7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP3yj1ZywkPY",
        "outputId": "dbe1bc2e-a9ca-42ae-dc02-c11429c78c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|       _c0|               _c1|               _c2|               _c3|               _c4|               _c5|     _c6|      _c7|         _c8|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Dividends|Stock Splits|\n",
            "|2013-11-21|11.936270713806152|12.002909660339355|11.850031852722168|11.955870628356934| 7.617593288421631|15586647|      0.0|         0.0|\n",
            "|2013-11-22|11.838272094726562|12.049949645996094|11.556035041809082|11.877471923828125| 7.567643165588379|30884260|      0.0|         0.0|\n",
            "|2013-11-25|11.916670799255371|11.951951026916504|11.591315269470215| 11.77163314819336| 7.500205039978027|17439983|      0.0|         0.0|\n",
            "|2013-11-26| 11.69323444366455|12.104828834533691| 11.63443374633789|11.720672607421875| 7.467736721038818|26836513|      0.0|         0.0|\n",
            "|2013-11-27|11.752033233642578|12.089150428771973|11.752033233642578|11.995071411132812|7.6425700187683105|20696140|      0.0|         0.0|\n",
            "|2013-11-28| 12.07738971710205|  12.1832275390625|11.881391525268555|12.034270286560059| 7.667546272277832|10971546|      0.0|         0.0|\n",
            "|2013-11-29|12.034270286560059|12.167549133300781|11.991150856018066| 12.15186882019043| 7.742466926574707|15718536|      0.0|         0.0|\n",
            "|2013-12-02|11.995071411132812|12.222428321838379|11.799073219299316|11.881391525268555|7.5701422691345215|30802116|      0.0|         0.0|\n",
            "|2013-12-03|11.763792991638184|11.850031852722168|11.497236251831055| 11.60699462890625| 7.400361061096191|19973429| 0.008114|         0.0|\n",
            "|2013-12-04|11.661873817443848| 11.69715404510498|11.387476921081543|11.454115867614746|7.3028950691223145|21362984|      0.0|         0.0|\n",
            "|2013-12-05|11.661873817443848| 11.69715404510498|11.289478302001953|11.375717163085938|  7.25290060043335|27727849|      0.0|         0.0|\n",
            "|2013-12-06|11.461956024169922| 11.50507640838623|11.262038230895996|11.367877006530762|7.2479071617126465|22152789|      0.0|         0.0|\n",
            "|2013-12-09|11.367877006530762|11.481555938720703|11.277717590332031|11.422757148742676|  7.28289794921875|15305266|      0.0|         0.0|\n",
            "|2013-12-10|11.367877006530762|11.399236679077148|11.262038230895996|11.285557746887207| 7.195419788360596|13179478|      0.0|         0.0|\n",
            "|2013-12-11|11.305157661437988|11.312997817993164|11.003320693969727| 11.03860092163086| 7.037967205047607|17765752|      0.0|         0.0|\n",
            "|2013-12-12|11.089559555053711|11.156200408935547|10.897481918334961| 11.13267993927002| 7.097955226898193|22719376|      0.0|         0.0|\n",
            "|2013-12-13|11.171878814697266|11.262038230895996|11.089559555053711|11.183638572692871| 7.130437850952148|18547903|      0.0|         0.0|\n",
            "|2013-12-16|11.195399284362793|11.340437889099121|11.124839782714844| 11.25811767578125| 7.177923202514648|20522924|      0.0|         0.0|\n",
            "|2013-12-17|11.269878387451172|11.340437889099121|11.191478729248047|11.242439270019531| 7.167929649353027|20344606|      0.0|         0.0|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header', True).csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSZ_6e0Uw02d",
        "outputId": "ed8e5188-d9aa-42aa-c4ea-d576d1788c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Dividends: string (nullable = true)\n",
            " |-- Stock Splits: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSRO9l0jw-aI",
        "outputId": "c68b6ed1-af00-4a64-bd4b-f87fdb2652fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Dividends|Stock Splits|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|2013-11-21|11.936270713806152|12.002909660339355|11.850031852722168|11.955870628356934| 7.617593288421631|15586647|      0.0|         0.0|\n",
            "|2013-11-22|11.838272094726562|12.049949645996094|11.556035041809082|11.877471923828125| 7.567643165588379|30884260|      0.0|         0.0|\n",
            "|2013-11-25|11.916670799255371|11.951951026916504|11.591315269470215| 11.77163314819336| 7.500205039978027|17439983|      0.0|         0.0|\n",
            "|2013-11-26| 11.69323444366455|12.104828834533691| 11.63443374633789|11.720672607421875| 7.467736721038818|26836513|      0.0|         0.0|\n",
            "|2013-11-27|11.752033233642578|12.089150428771973|11.752033233642578|11.995071411132812|7.6425700187683105|20696140|      0.0|         0.0|\n",
            "|2013-11-28| 12.07738971710205|  12.1832275390625|11.881391525268555|12.034270286560059| 7.667546272277832|10971546|      0.0|         0.0|\n",
            "|2013-11-29|12.034270286560059|12.167549133300781|11.991150856018066| 12.15186882019043| 7.742466926574707|15718536|      0.0|         0.0|\n",
            "|2013-12-02|11.995071411132812|12.222428321838379|11.799073219299316|11.881391525268555|7.5701422691345215|30802116|      0.0|         0.0|\n",
            "|2013-12-03|11.763792991638184|11.850031852722168|11.497236251831055| 11.60699462890625| 7.400361061096191|19973429| 0.008114|         0.0|\n",
            "|2013-12-04|11.661873817443848| 11.69715404510498|11.387476921081543|11.454115867614746|7.3028950691223145|21362984|      0.0|         0.0|\n",
            "|2013-12-05|11.661873817443848| 11.69715404510498|11.289478302001953|11.375717163085938|  7.25290060043335|27727849|      0.0|         0.0|\n",
            "|2013-12-06|11.461956024169922| 11.50507640838623|11.262038230895996|11.367877006530762|7.2479071617126465|22152789|      0.0|         0.0|\n",
            "|2013-12-09|11.367877006530762|11.481555938720703|11.277717590332031|11.422757148742676|  7.28289794921875|15305266|      0.0|         0.0|\n",
            "|2013-12-10|11.367877006530762|11.399236679077148|11.262038230895996|11.285557746887207| 7.195419788360596|13179478|      0.0|         0.0|\n",
            "|2013-12-11|11.305157661437988|11.312997817993164|11.003320693969727| 11.03860092163086| 7.037967205047607|17765752|      0.0|         0.0|\n",
            "|2013-12-12|11.089559555053711|11.156200408935547|10.897481918334961| 11.13267993927002| 7.097955226898193|22719376|      0.0|         0.0|\n",
            "|2013-12-13|11.171878814697266|11.262038230895996|11.089559555053711|11.183638572692871| 7.130437850952148|18547903|      0.0|         0.0|\n",
            "|2013-12-16|11.195399284362793|11.340437889099121|11.124839782714844| 11.25811767578125| 7.177923202514648|20522924|      0.0|         0.0|\n",
            "|2013-12-17|11.269878387451172|11.340437889099121|11.191478729248047|11.242439270019531| 7.167929649353027|20344606|      0.0|         0.0|\n",
            "|2013-12-18|11.340437889099121|11.473715782165527|11.218918800354004|11.305157661437988| 7.207915782928467|22392842|      0.0|         0.0|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header', True).option('inferSchema', True).csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G987M6nKxHOK",
        "outputId": "84258f57-56a8-43fc-92ac-c61f5c39b833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Dividends: double (nullable = true)\n",
            " |-- Stock Splits: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais **transforma√ß√µes** e **a√ß√µes** dispon√≠veis no Apache Spark DataFrame\n",
        "Depois de criar o DataFrame a partir do arquivo CSV, podemos aplicar todas as **transforma√ß√µes** e **a√ß√µes** de suporte ao DataFrame. Abaixo est√£o algumas das principais **transforma√ß√µes** e **a√ß√µes** dispon√≠veis no Apache Spark DataFrame.\n",
        "\n",
        "\\\n",
        "## Transforma√ß√µes\n",
        "\n",
        "\\\n",
        "**```select```**: Seleciona um conjunto espec√≠fico de colunas.\n",
        "\n",
        "```python\n",
        "df.select(\"coluna1\", \"coluna2\")\n",
        "```\n",
        "\\\n",
        "**```filter```**: Filtra as linhas com base em uma condi√ß√£o.\n",
        "\n",
        "```python\n",
        "df.filter(df[\"coluna\"] > 10)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```groupBy```**: Agrupa o DataFrame por uma ou mais colunas.\n",
        "\n",
        "```python\n",
        "df.groupBy(\"coluna1\").agg({\"coluna2\": \"sum\"})\n",
        "```\n",
        "\n",
        "\\\n",
        "**```withColumn```**: Adiciona ou substitui uma coluna.\n",
        "```python\n",
        "df.withColumn(\"nova_coluna\", df[\"coluna\"] * 2)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```join```**: Realiza uma jun√ß√£o entre dois DataFrames.\n",
        "```python\n",
        "df1.join(df2, df1[\"coluna1\"] == df2[\"coluna2\"], \"inner\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```orderBy```**: Ordena o DataFrame com base em uma ou mais colunas.\n",
        "\n",
        "```python\n",
        "df.orderBy(\"coluna1\", ascending=False)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```drop```**: Remove uma coluna do DataFrame.\n",
        "```python\n",
        "df.drop(\"coluna\")\n",
        "```\n",
        "\\\n",
        "**```distinct```**: Retorna as linhas distintas do DataFrame.\n",
        "```python\n",
        "df.distinct()\n",
        "```\n",
        "\\\n",
        "##A√ß√µes\n",
        "\\\n",
        "**```show```**: Exibe as primeiras linhas do DataFrame.\n",
        "```python\n",
        "df.show()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```count```**: Retorna o n√∫mero de linhas no DataFrame.\n",
        "```python\n",
        "df.count()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```collect```**: Retorna todas as linhas do DataFrame como uma lista no programa driver.\n",
        "```python\n",
        "df.collect()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```take```**: Retorna as primeiras **```n```** linhas do DataFrame.\n",
        "```python\n",
        "df.take(5)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```describe```**: Calcula estat√≠sticas descritivas para colunas num√©ricas.\n",
        "```python\n",
        "df.describe(\"coluna1\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```printSchema```**: Exibe o esquema do DataFrame.\n",
        "```python\n",
        "df.printSchema()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```write```**: Escreve o DataFrame em fontes externas.\n",
        "```python\n",
        "df.write.format(\"parquet\").save(\"caminho/do/arquivo\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```save```**: Salva o DataFrame em um formato espec√≠fico (por exemplo, parquet, CSV).\n",
        "```python\n",
        "df.write.save(\"caminho/do/arquivo\", format=\"parquet\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "cxeGu9psxhcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Exerc√≠cio: An√°lise de informa√ß√µes dos jogadores da NBA ‚õπÔ∏è\n",
        "Responda as seguintes quest√µes a partir dos dados de jogadores da NBA [NBA Players stats since 1950](https://www.kaggle.com/datasets/drgilermo/nba-players-stats).\n",
        "\n",
        "1. Qual a m√©dia de altura e de peso dos jogadores?\n",
        "2. Crie uma nova coluna com o $IMC = \\frac{peso}{altura¬≤}$.\n",
        "3. Qual a maior altura, o maior peso, a menor altura e o menor peso?\n",
        "4. Em qual estado americano nasceu o maior n√∫mero de jogadores?\n",
        "5. Qual ou quais s√£o os jogadores mais altos?"
      ],
      "metadata": {
        "id": "ykubruifx3N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head Players.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSzuRyT94fH4",
        "outputId": "1660ed67-3341-4c35-9db3-64e7c3dff9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",Player,height,weight,collage,born,birth_city,birth_state\r\n",
            "0,Curly Armstrong,180,77,Indiana University,1918,,\r\n",
            "1,Cliff Barker,188,83,University of Kentucky,1921,Yorktown,Indiana\r\n",
            "2,Leo Barnhorst,193,86,University of Notre Dame,1924,,\r\n",
            "3,Ed Bartels,196,88,North Carolina State University,1925,,\r\n",
            "4,Ralph Beard,178,79,University of Kentucky,1927,Hardinsburg,Kentucky\r\n",
            "5,Gene Berce,180,79,Marquette University,1926,,\r\n",
            "6,Charlie Black,196,90,University of Kansas,1921,Arco,Idaho\r\n",
            "7,Nelson Bobb,183,77,Temple University,1924,Philadelphia,Pennsylvania\r\n",
            "8,Jake Bornheimer,196,90,Muhlenberg College,1927,New Brunswick,New Jersey\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df = spark.read.option('header', True).option('inferSchema', True).csv(\"Players.csv\")\n",
        "players_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlZ-HMhq4mpo",
        "outputId": "ad348488-ca21-42f3-87b1-0123da7590a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- Player: string (nullable = true)\n",
            " |-- height: integer (nullable = true)\n",
            " |-- weight: integer (nullable = true)\n",
            " |-- collage: string (nullable = true)\n",
            " |-- born: integer (nullable = true)\n",
            " |-- birth_city: string (nullable = true)\n",
            " |-- birth_state: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG8dzUTR4wZM",
        "outputId": "19ba7575-a940-498d-f939-2daa7fbc17ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "|_c0|          Player|height|weight|             collage|born|   birth_city| birth_state|\n",
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "|  0| Curly Armstrong|   180|    77|  Indiana University|1918|         NULL|        NULL|\n",
            "|  1|    Cliff Barker|   188|    83|University of Ken...|1921|     Yorktown|     Indiana|\n",
            "|  2|   Leo Barnhorst|   193|    86|University of Not...|1924|         NULL|        NULL|\n",
            "|  3|      Ed Bartels|   196|    88|North Carolina St...|1925|         NULL|        NULL|\n",
            "|  4|     Ralph Beard|   178|    79|University of Ken...|1927|  Hardinsburg|    Kentucky|\n",
            "|  5|      Gene Berce|   180|    79|Marquette University|1926|         NULL|        NULL|\n",
            "|  6|   Charlie Black|   196|    90|University of Kansas|1921|         Arco|       Idaho|\n",
            "|  7|     Nelson Bobb|   183|    77|   Temple University|1924| Philadelphia|Pennsylvania|\n",
            "|  8| Jake Bornheimer|   196|    90|  Muhlenberg College|1927|New Brunswick|  New Jersey|\n",
            "|  9|    Vince Boryla|   196|    95|University of Denver|1927| East Chicago|     Indiana|\n",
            "| 10|       Don Boven|   193|    95|Western Michigan ...|1925|    Kalamazoo|    Michigan|\n",
            "| 11|   Harry Boykoff|   208|   102|St. John's Univer...|1922|     Brooklyn|    New York|\n",
            "| 12|     Joe Bradley|   190|    79|Oklahoma State Un...|1928|   Washington|    Oklahoma|\n",
            "| 13|     Bob Brannum|   196|    97|Michigan State Un...|1925|         NULL|        NULL|\n",
            "| 14|      Carl Braun|   196|    81|  Colgate University|1927|     Brooklyn|    New York|\n",
            "| 15|   Frankie Brian|   185|    81|Louisiana State U...|1923|      Zachary|   Louisiana|\n",
            "| 16|Price Brookfield|   193|    83|West Texas A&M Un...|1920|     Floydada|       Texas|\n",
            "| 17|       Bob Brown|   193|    92|    Miami University|1923|   Versailles|        Ohio|\n",
            "| 18|      Jim Browne|   208|   106|                NULL|1930|   Midlothian|    Illinois|\n",
            "| 19|      Walt Budko|   196|    99| Columbia University|1925|      Kearney|  New Jersey|\n",
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1Ô∏è‚É£. Qual a m√©dia de altura e de peso dos jogadores?"
      ],
      "metadata": {
        "id": "OevbePGT44Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean, udf, round"
      ],
      "metadata": {
        "id": "NhP-59Sq5SMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.select(round(mean(col('height')/100), 2)).collect()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jTq8Djg5FcN",
        "outputId": "b7bfc18a-7f7a-419a-8471-5605663871c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.99"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solu√ß√£o alternativa\n",
        "\n",
        "from pyspark.sql.functions import mean, avg\n",
        "\n",
        "players_df.select( avg( players_df['height'] ),  mean( players_df.weight )  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcp8mBdA56Tq",
        "outputId": "6989f9ff-effe-44c8-cbb4-42be4a9009ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------+\n",
            "|       avg(height)|      avg(weight)|\n",
            "+------------------+-----------------+\n",
            "|198.70492221372098|94.78321856669217|\n",
            "+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.agg( { 'height': 'avg', 'weight':'avg' } ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o0XSR-Q6V89",
        "outputId": "bba5e8d6-7062-4db8-f262-5afb0200d315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+------------------+\n",
            "|      avg(weight)|       avg(height)|\n",
            "+-----------------+------------------+\n",
            "|94.78321856669217|198.70492221372098|\n",
            "+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean, udf\n",
        "\n",
        "media_altura = df.select(round(mean(col('height')/100), 2)).collect()[0][0]\n",
        "media_peso = df.select(round(mean(col('weight')), 2)).collect()[0][0]\n",
        "\n",
        "print(\"\"\"M√©dia de altura dos jogadores: {}m \\nM√©dia de peso dos jogadores: {}kg\n",
        "      \"\"\".format(media_altura, media_peso))"
      ],
      "metadata": {
        "id": "XLGSFi3I45Do"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}