{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielgsv/ClinicaBack/blob/master/%5BMBA_ES_27%5D_Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbw_w-qFariJ",
        "outputId": "a7e02f47-b831-4df8-fc38-cc42e745bd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dos datasets e arquivos de exemplos\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/cars.json\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/cidades.json\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/dados+cota%C3%A7%C3%B5es/BBDC4_10y.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/Players.csv\n",
        "!wget https://datasets-aulas.s3.amazonaws.com/czech-financial-dataset-real-anonymized-transactions.zip\n",
        "!wget https://raw.githubusercontent.com/spark-examples/pyspark-examples/master/resources/zipcodes.json\n",
        "!unzip czech-financial-dataset-real-anonymized-transactions.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQyHz4rnbFnq",
        "outputId": "36e8f3c4-78e3-497f-b980-058c2f8bc085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-28 23:18:28--  https://datasets-aulas.s3.amazonaws.com/cars.json\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1580 (1.5K) [application/json]\n",
            "Saving to: ‘cars.json’\n",
            "\n",
            "cars.json           100%[===================>]   1.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:29 (42.8 MB/s) - ‘cars.json’ saved [1580/1580]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://datasets-aulas.s3.amazonaws.com/cidades.json\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6925 (6.8K) [application/json]\n",
            "Saving to: ‘cidades.json’\n",
            "\n",
            "cidades.json        100%[===================>]   6.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:29 (156 MB/s) - ‘cidades.json’ saved [6925/6925]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://datasets-aulas.s3.amazonaws.com/dados+cota%C3%A7%C3%B5es/BBDC4_10y.csv\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.234.241, 16.15.193.237, 52.216.130.115, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.234.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299951 (293K) [text/csv]\n",
            "Saving to: ‘BBDC4_10y.csv’\n",
            "\n",
            "BBDC4_10y.csv       100%[===================>] 292.92K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-10-28 23:18:29 (2.51 MB/s) - ‘BBDC4_10y.csv’ saved [299951/299951]\n",
            "\n",
            "--2024-10-28 23:18:29--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘adult.data’\n",
            "\n",
            "adult.data              [      <=>           ]   3.79M  3.63MB/s    in 1.0s    \n",
            "\n",
            "2024-10-28 23:18:31 (3.63 MB/s) - ‘adult.data’ saved [3974305]\n",
            "\n",
            "--2024-10-28 23:18:31--  https://datasets-aulas.s3.amazonaws.com/Players.csv\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.33.220, 52.216.49.113, 54.231.228.105, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.33.220|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 281111 (275K) [text/csv]\n",
            "Saving to: ‘Players.csv’\n",
            "\n",
            "Players.csv         100%[===================>] 274.52K   499KB/s    in 0.6s    \n",
            "\n",
            "2024-10-28 23:18:32 (499 KB/s) - ‘Players.csv’ saved [281111/281111]\n",
            "\n",
            "--2024-10-28 23:18:32--  https://datasets-aulas.s3.amazonaws.com/czech-financial-dataset-real-anonymized-transactions.zip\n",
            "Resolving datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)... 52.217.33.220, 52.216.49.113, 54.231.228.105, ...\n",
            "Connecting to datasets-aulas.s3.amazonaws.com (datasets-aulas.s3.amazonaws.com)|52.217.33.220|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35693438 (34M) [application/zip]\n",
            "Saving to: ‘czech-financial-dataset-real-anonymized-transactions.zip’\n",
            "\n",
            "czech-financial-dat 100%[===================>]  34.04M  38.6MB/s    in 0.9s    \n",
            "\n",
            "2024-10-28 23:18:33 (38.6 MB/s) - ‘czech-financial-dataset-real-anonymized-transactions.zip’ saved [35693438/35693438]\n",
            "\n",
            "--2024-10-28 23:18:33--  https://raw.githubusercontent.com/spark-examples/pyspark-examples/master/resources/zipcodes.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7331 (7.2K) [text/plain]\n",
            "Saving to: ‘zipcodes.json’\n",
            "\n",
            "zipcodes.json       100%[===================>]   7.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 23:18:33 (51.2 MB/s) - ‘zipcodes.json’ saved [7331/7331]\n",
            "\n",
            "Archive:  czech-financial-dataset-real-anonymized-transactions.zip\n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/datapackage.json  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/account.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/card.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/client.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/disp.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/district.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/loan.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/order.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/data/trans.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/account.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/card.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/client.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/data map.gif  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/disp.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/district.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/loan.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/order.csv  \n",
            "  inflating: lpetrocelli-czech-financial-dataset-real-anonymized-transactions/original/trans.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "# 👨‍🔬 Testando a instalação\n",
        "Agora, podemos importar **```SparkSession```** de **```pyspark.sql```** e criar uma **```SparkSession```**. **```SparkSession```** foi introduzido na versão **Spark 2.0** e é um ponto de entrada para a funcionalidade Spark subjacente para criar programaticamente **RDDs**, **DataFrames** e **DataSets**.\n",
        "\n",
        "Você pode dar um nome à sessão usando **```appName()```** e adicionando algumas configurações com **```config()```** se desejar."
      ],
      "metadata": {
        "id": "vZNT_lgfcL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "qP5P5qpucMo3",
        "outputId": "460c37bd-e0a1-4478-d036-120c30d610a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f41cde65fc0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b5571be3fd6e:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSession\n",
        "Desde o Spark 2.0, o **```SparkSession```** se tornou um ponto de entrada para o PySpark trabalhar com **```RDD```** e **```DataFrame```**. Antes da versão 2.0, o **```SparkContext```** costumava ser um ponto de entrada. O **```SparkSession```** é uma classe combinada para todos os diferentes contextos que tínhamos antes do lançamento 2.0 ( **```SparkContext```**, **```StreamingContext```**, **```SQLContext```**, **```HiveContext```**, etc).\n",
        "\n",
        "Para criar **```SparkSession```** programaticamente no PySpark, você precisa usar o método padrão do construtor **```builder()```**. O método **```getOrCreate()```** retorna um **```SparkSession```** já existente.; se não existir, cria uma nova **```SparkSession```**.\n",
        "\n",
        "**```master()```** – Se você estiver executando no cluster, você precisará usar seu nome mestre como argumento para **```master()```**. Normalmente, será **```yarn```** ou **```mesos```**, dependendo da configuração do seu cluster. Use **```local[x]```** ao executar no modo **Standalone** onde **```x```** deve ser um valor inteiro e deve ser maior que **```0```**. Isso representa quantas partições ele deve criar ao usar um **```RDD```**, **```DataFrame```** ou **```Dataset```**. Idealmente, o valor **```x```** deve ser o número de núcleos de CPU que você possui.\n",
        "\n",
        "**```appName()```** – Usado para definir o nome do aplicativo.\n",
        "\n",
        "**```getOrCreate()```** – Retorna um objeto **```SparkSession```** se já existir e cria um novo se não existir.\n",
        "\n",
        "\\\n",
        "## Usando **```SparkConfig```**\n",
        "Se você quiser definir algumas configurações para **```SparkSession```**, use o método **```config()```**.\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[1]\") \\\n",
        "      .appName(\"Exemplo\") \\\n",
        "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
        "      .getOrCreate()\n",
        "```"
      ],
      "metadata": {
        "id": "MJFRPX-6dIlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primeiro exemplo: Total de países distintos presentes no dataset\n",
        "🇨🇺 🇮🇳 🇲🇽 🇿🇦 🇵🇷 🇭🇳 🏴󠁧󠁢󠁥󠁮󠁧󠁿 🇨🇦 🇩🇪 🇮🇷 🇮🇹 🇵🇱 🇨🇴 🇰🇭 🇹🇭 🇱🇦 🇭🇹 🇬🇹 🇵🇪 🏴󠁧󠁢󠁳󠁣󠁴󠁿 🇹🇹 🇬🇷 🇳🇮 🇳🇱 🇺🇸 🇯🇲 🇵🇭 🇪🇨 🇹🇼 🇵🇹 🇩🇴 🇸🇻 🇫🇷 🇨🇳 🇯🇵 🇭🇷🇸🇮🇲🇰🇧🇦🇲🇪 🇬🇺 🇻🇳 🇭🇰 🇮🇪 🇭🇺"
      ],
      "metadata": {
        "id": "wP7sl-CMdjKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Configuração do Spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Total de países distintos presentes no dataset\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Carrega o conjunto de dados a partir do arquivo CSV\n",
        "adultDatasetRDD = spark.sparkContext.textFile(\"adult.data\")\n",
        "\n",
        "# Divide cada linha do conjunto de dados por vírgula, removendo linhas vazias\n",
        "splitAdultDatasetRDD = adultDatasetRDD.filter(lambda linha: linha != '').map(lambda linha: linha.split(\",\"))\n",
        "\n",
        "# Seleciona a coluna correspondente ao país (índice 13) e obtém valores distintos\n",
        "paises = splitAdultDatasetRDD.map(lambda linha: linha[13]).distinct()\n",
        "\n",
        "# Conta a quantidade de países distintos\n",
        "count_paises = paises.count()\n",
        "\n",
        "# Coleta os países distintos como uma lista\n",
        "paises_list = paises.collect()\n",
        "\n",
        "# Imprime o resultado\n",
        "print(f\"Quantidade de países distintos: {count_paises}\")\n",
        "print(\"Países distintos:\", paises_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIkgPwqcdjzz",
        "outputId": "d5634e2b-ddae-4a94-b960-7ed00c6c56de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de países distintos: 42\n",
            "Países distintos: [' Cuba', ' India', ' Mexico', ' South', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Italy', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Laos', ' Haiti', ' Guatemala', ' Peru', ' Scotland', ' Trinadad&Tobago', ' Greece', ' Nicaragua', ' Holand-Netherlands', ' United-States', ' Jamaica', ' ?', ' Philippines', ' Ecuador', ' Taiwan', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' China', ' Japan', ' Yugoslavia', ' Outlying-US(Guam-USVI-etc)', ' Vietnam', ' Hong', ' Ireland', ' Hungary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "# Dataframes\n",
        "O Apache Spark **DataFrame** é uma estrutura de dados tabular distribuída, oferecendo uma interface de alto nível para manipulação eficiente de grandes conjuntos de dados. Inspirado pela simplicidade das tabelas em bancos de dados relacionais, o DataFrame do Spark simplifica operações complexas, permitindo análises poderosas em ambientes distribuídos.\n",
        "\n",
        "Sua abstração intuitiva facilita a execução de transformações e consultas SQL, proporcionando flexibilidade e desempenho otimizado para processamento de dados em escala. Com suporte a diversas fontes de dados e integração perfeita com o ecossistema Spark, o DataFrame se tornou uma ferramenta fundamental para cientistas de dados e engenheiros de dados na construção de pipelines de análise robustos e eficientes.\n",
        "\n",
        "Definição de um [DataFrame](https://www.databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html) pela Databricks:\n",
        "> DataFrame é uma coleção distribuída de dados organizados em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou a um quadro de dados em R/Python, mas com otimizações mais ricas nos bastidores. DataFrames podem ser construídos a partir de uma ampla variedade de fontes, como arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes. – Databricks"
      ],
      "metadata": {
        "id": "lAbcSJX1vEEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "## Criando um DataFrame\n",
        "A maneira mais simples de criar um DataFrame é a partir de uma lista de dados Python. O DataFrame também pode ser criado a partir de um RDD e lendo arquivos de diversas fontes."
      ],
      "metadata": {
        "id": "MWrf0t0-vH_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados = [('João','','Silva','1991-04-01','M',3000),\n",
        "  ('Miguel','Ribeiro','','2000-05-19','M',4000),\n",
        "  ('Roberto','','Ferreira','1978-09-05','M',4000),\n",
        "  ('Maria','Ana','Oliveira','1967-12-01','F',4000),\n",
        "  ('Jéssica','Mariana','Brito','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"primeiro_nome\",\"nome_meio\",\"sobrenome\",\"data_nascimento\",\"genero\",\"salario\"]\n",
        "df = spark.createDataFrame(data=dados, schema = columns)"
      ],
      "metadata": {
        "id": "c3DpX2YMvMUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8nKO9UwvdLN",
        "outputId": "6f024b40-cec8-4b8c-bbbe-8c09b3b21aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+---------+---------------+------+-------+\n",
            "|primeiro_nome|nome_meio|sobrenome|data_nascimento|genero|salario|\n",
            "+-------------+---------+---------+---------------+------+-------+\n",
            "|         João|         |    Silva|     1991-04-01|     M|   3000|\n",
            "|       Miguel|  Ribeiro|         |     2000-05-19|     M|   4000|\n",
            "|      Roberto|         | Ferreira|     1978-09-05|     M|   4000|\n",
            "|        Maria|      Ana| Oliveira|     1967-12-01|     F|   4000|\n",
            "|      Jéssica|  Mariana|    Brito|     1980-02-17|     F|     -1|\n",
            "+-------------+---------+---------+---------------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "## Lendo um arquivo CSV no DataFrame\n",
        "PySpark fornece **```csv(\"path\")```** no DataFrameReader para ler um arquivo CSV no PySpark DataFrame e **```dataframeObj.write.csv(\"path\")```** para salvar ou gravar no arquivo CSV.\n",
        "\n",
        "Este exemplo lê os dados nas colunas DataFrame **```_c0```** para a primeira coluna e **```_c1```** para a segunda e assim por diante. e por padrão o tipo de dados para todas essas colunas é tratado como String."
      ],
      "metadata": {
        "id": "B04wedpZvsFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[2]\")\\\n",
        "        .appName(\"Lendo um arquivo CSV no DataFrame\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wXhZ9YxvvcA",
        "outputId": "c9b24732-8b1d-41d9-d40a-6555e398b7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP3yj1ZywkPY",
        "outputId": "dbe1bc2e-a9ca-42ae-dc02-c11429c78c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|       _c0|               _c1|               _c2|               _c3|               _c4|               _c5|     _c6|      _c7|         _c8|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Dividends|Stock Splits|\n",
            "|2013-11-21|11.936270713806152|12.002909660339355|11.850031852722168|11.955870628356934| 7.617593288421631|15586647|      0.0|         0.0|\n",
            "|2013-11-22|11.838272094726562|12.049949645996094|11.556035041809082|11.877471923828125| 7.567643165588379|30884260|      0.0|         0.0|\n",
            "|2013-11-25|11.916670799255371|11.951951026916504|11.591315269470215| 11.77163314819336| 7.500205039978027|17439983|      0.0|         0.0|\n",
            "|2013-11-26| 11.69323444366455|12.104828834533691| 11.63443374633789|11.720672607421875| 7.467736721038818|26836513|      0.0|         0.0|\n",
            "|2013-11-27|11.752033233642578|12.089150428771973|11.752033233642578|11.995071411132812|7.6425700187683105|20696140|      0.0|         0.0|\n",
            "|2013-11-28| 12.07738971710205|  12.1832275390625|11.881391525268555|12.034270286560059| 7.667546272277832|10971546|      0.0|         0.0|\n",
            "|2013-11-29|12.034270286560059|12.167549133300781|11.991150856018066| 12.15186882019043| 7.742466926574707|15718536|      0.0|         0.0|\n",
            "|2013-12-02|11.995071411132812|12.222428321838379|11.799073219299316|11.881391525268555|7.5701422691345215|30802116|      0.0|         0.0|\n",
            "|2013-12-03|11.763792991638184|11.850031852722168|11.497236251831055| 11.60699462890625| 7.400361061096191|19973429| 0.008114|         0.0|\n",
            "|2013-12-04|11.661873817443848| 11.69715404510498|11.387476921081543|11.454115867614746|7.3028950691223145|21362984|      0.0|         0.0|\n",
            "|2013-12-05|11.661873817443848| 11.69715404510498|11.289478302001953|11.375717163085938|  7.25290060043335|27727849|      0.0|         0.0|\n",
            "|2013-12-06|11.461956024169922| 11.50507640838623|11.262038230895996|11.367877006530762|7.2479071617126465|22152789|      0.0|         0.0|\n",
            "|2013-12-09|11.367877006530762|11.481555938720703|11.277717590332031|11.422757148742676|  7.28289794921875|15305266|      0.0|         0.0|\n",
            "|2013-12-10|11.367877006530762|11.399236679077148|11.262038230895996|11.285557746887207| 7.195419788360596|13179478|      0.0|         0.0|\n",
            "|2013-12-11|11.305157661437988|11.312997817993164|11.003320693969727| 11.03860092163086| 7.037967205047607|17765752|      0.0|         0.0|\n",
            "|2013-12-12|11.089559555053711|11.156200408935547|10.897481918334961| 11.13267993927002| 7.097955226898193|22719376|      0.0|         0.0|\n",
            "|2013-12-13|11.171878814697266|11.262038230895996|11.089559555053711|11.183638572692871| 7.130437850952148|18547903|      0.0|         0.0|\n",
            "|2013-12-16|11.195399284362793|11.340437889099121|11.124839782714844| 11.25811767578125| 7.177923202514648|20522924|      0.0|         0.0|\n",
            "|2013-12-17|11.269878387451172|11.340437889099121|11.191478729248047|11.242439270019531| 7.167929649353027|20344606|      0.0|         0.0|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header', True).csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSZ_6e0Uw02d",
        "outputId": "ed8e5188-d9aa-42aa-c4ea-d576d1788c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Dividends: string (nullable = true)\n",
            " |-- Stock Splits: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSRO9l0jw-aI",
        "outputId": "c68b6ed1-af00-4a64-bd4b-f87fdb2652fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Dividends|Stock Splits|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "|2013-11-21|11.936270713806152|12.002909660339355|11.850031852722168|11.955870628356934| 7.617593288421631|15586647|      0.0|         0.0|\n",
            "|2013-11-22|11.838272094726562|12.049949645996094|11.556035041809082|11.877471923828125| 7.567643165588379|30884260|      0.0|         0.0|\n",
            "|2013-11-25|11.916670799255371|11.951951026916504|11.591315269470215| 11.77163314819336| 7.500205039978027|17439983|      0.0|         0.0|\n",
            "|2013-11-26| 11.69323444366455|12.104828834533691| 11.63443374633789|11.720672607421875| 7.467736721038818|26836513|      0.0|         0.0|\n",
            "|2013-11-27|11.752033233642578|12.089150428771973|11.752033233642578|11.995071411132812|7.6425700187683105|20696140|      0.0|         0.0|\n",
            "|2013-11-28| 12.07738971710205|  12.1832275390625|11.881391525268555|12.034270286560059| 7.667546272277832|10971546|      0.0|         0.0|\n",
            "|2013-11-29|12.034270286560059|12.167549133300781|11.991150856018066| 12.15186882019043| 7.742466926574707|15718536|      0.0|         0.0|\n",
            "|2013-12-02|11.995071411132812|12.222428321838379|11.799073219299316|11.881391525268555|7.5701422691345215|30802116|      0.0|         0.0|\n",
            "|2013-12-03|11.763792991638184|11.850031852722168|11.497236251831055| 11.60699462890625| 7.400361061096191|19973429| 0.008114|         0.0|\n",
            "|2013-12-04|11.661873817443848| 11.69715404510498|11.387476921081543|11.454115867614746|7.3028950691223145|21362984|      0.0|         0.0|\n",
            "|2013-12-05|11.661873817443848| 11.69715404510498|11.289478302001953|11.375717163085938|  7.25290060043335|27727849|      0.0|         0.0|\n",
            "|2013-12-06|11.461956024169922| 11.50507640838623|11.262038230895996|11.367877006530762|7.2479071617126465|22152789|      0.0|         0.0|\n",
            "|2013-12-09|11.367877006530762|11.481555938720703|11.277717590332031|11.422757148742676|  7.28289794921875|15305266|      0.0|         0.0|\n",
            "|2013-12-10|11.367877006530762|11.399236679077148|11.262038230895996|11.285557746887207| 7.195419788360596|13179478|      0.0|         0.0|\n",
            "|2013-12-11|11.305157661437988|11.312997817993164|11.003320693969727| 11.03860092163086| 7.037967205047607|17765752|      0.0|         0.0|\n",
            "|2013-12-12|11.089559555053711|11.156200408935547|10.897481918334961| 11.13267993927002| 7.097955226898193|22719376|      0.0|         0.0|\n",
            "|2013-12-13|11.171878814697266|11.262038230895996|11.089559555053711|11.183638572692871| 7.130437850952148|18547903|      0.0|         0.0|\n",
            "|2013-12-16|11.195399284362793|11.340437889099121|11.124839782714844| 11.25811767578125| 7.177923202514648|20522924|      0.0|         0.0|\n",
            "|2013-12-17|11.269878387451172|11.340437889099121|11.191478729248047|11.242439270019531| 7.167929649353027|20344606|      0.0|         0.0|\n",
            "|2013-12-18|11.340437889099121|11.473715782165527|11.218918800354004|11.305157661437988| 7.207915782928467|22392842|      0.0|         0.0|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header', True).option('inferSchema', True).csv(\"BBDC4_10y.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G987M6nKxHOK",
        "outputId": "84258f57-56a8-43fc-92ac-c61f5c39b833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Dividends: double (nullable = true)\n",
            " |-- Stock Splits: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais **transformações** e **ações** disponíveis no Apache Spark DataFrame\n",
        "Depois de criar o DataFrame a partir do arquivo CSV, podemos aplicar todas as **transformações** e **ações** de suporte ao DataFrame. Abaixo estão algumas das principais **transformações** e **ações** disponíveis no Apache Spark DataFrame.\n",
        "\n",
        "\\\n",
        "## Transformações\n",
        "\n",
        "\\\n",
        "**```select```**: Seleciona um conjunto específico de colunas.\n",
        "\n",
        "```python\n",
        "df.select(\"coluna1\", \"coluna2\")\n",
        "```\n",
        "\\\n",
        "**```filter```**: Filtra as linhas com base em uma condição.\n",
        "\n",
        "```python\n",
        "df.filter(df[\"coluna\"] > 10)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```groupBy```**: Agrupa o DataFrame por uma ou mais colunas.\n",
        "\n",
        "```python\n",
        "df.groupBy(\"coluna1\").agg({\"coluna2\": \"sum\"})\n",
        "```\n",
        "\n",
        "\\\n",
        "**```withColumn```**: Adiciona ou substitui uma coluna.\n",
        "```python\n",
        "df.withColumn(\"nova_coluna\", df[\"coluna\"] * 2)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```join```**: Realiza uma junção entre dois DataFrames.\n",
        "```python\n",
        "df1.join(df2, df1[\"coluna1\"] == df2[\"coluna2\"], \"inner\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```orderBy```**: Ordena o DataFrame com base em uma ou mais colunas.\n",
        "\n",
        "```python\n",
        "df.orderBy(\"coluna1\", ascending=False)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```drop```**: Remove uma coluna do DataFrame.\n",
        "```python\n",
        "df.drop(\"coluna\")\n",
        "```\n",
        "\\\n",
        "**```distinct```**: Retorna as linhas distintas do DataFrame.\n",
        "```python\n",
        "df.distinct()\n",
        "```\n",
        "\\\n",
        "##Ações\n",
        "\\\n",
        "**```show```**: Exibe as primeiras linhas do DataFrame.\n",
        "```python\n",
        "df.show()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```count```**: Retorna o número de linhas no DataFrame.\n",
        "```python\n",
        "df.count()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```collect```**: Retorna todas as linhas do DataFrame como uma lista no programa driver.\n",
        "```python\n",
        "df.collect()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```take```**: Retorna as primeiras **```n```** linhas do DataFrame.\n",
        "```python\n",
        "df.take(5)\n",
        "```\n",
        "\n",
        "\\\n",
        "**```describe```**: Calcula estatísticas descritivas para colunas numéricas.\n",
        "```python\n",
        "df.describe(\"coluna1\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```printSchema```**: Exibe o esquema do DataFrame.\n",
        "```python\n",
        "df.printSchema()\n",
        "```\n",
        "\n",
        "\\\n",
        "**```write```**: Escreve o DataFrame em fontes externas.\n",
        "```python\n",
        "df.write.format(\"parquet\").save(\"caminho/do/arquivo\")\n",
        "```\n",
        "\n",
        "\\\n",
        "**```save```**: Salva o DataFrame em um formato específico (por exemplo, parquet, CSV).\n",
        "```python\n",
        "df.write.save(\"caminho/do/arquivo\", format=\"parquet\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "cxeGu9psxhcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Exercício: Análise de informações dos jogadores da NBA ⛹️\n",
        "Responda as seguintes questões a partir dos dados de jogadores da NBA [NBA Players stats since 1950](https://www.kaggle.com/datasets/drgilermo/nba-players-stats).\n",
        "\n",
        "1. Qual a média de altura e de peso dos jogadores?\n",
        "2. Crie uma nova coluna com o $IMC = \\frac{peso}{altura²}$.\n",
        "3. Qual a maior altura, o maior peso, a menor altura e o menor peso?\n",
        "4. Em qual estado americano nasceu o maior número de jogadores?\n",
        "5. Qual ou quais são os jogadores mais altos?"
      ],
      "metadata": {
        "id": "ykubruifx3N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head Players.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSzuRyT94fH4",
        "outputId": "1660ed67-3341-4c35-9db3-64e7c3dff9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",Player,height,weight,collage,born,birth_city,birth_state\r\n",
            "0,Curly Armstrong,180,77,Indiana University,1918,,\r\n",
            "1,Cliff Barker,188,83,University of Kentucky,1921,Yorktown,Indiana\r\n",
            "2,Leo Barnhorst,193,86,University of Notre Dame,1924,,\r\n",
            "3,Ed Bartels,196,88,North Carolina State University,1925,,\r\n",
            "4,Ralph Beard,178,79,University of Kentucky,1927,Hardinsburg,Kentucky\r\n",
            "5,Gene Berce,180,79,Marquette University,1926,,\r\n",
            "6,Charlie Black,196,90,University of Kansas,1921,Arco,Idaho\r\n",
            "7,Nelson Bobb,183,77,Temple University,1924,Philadelphia,Pennsylvania\r\n",
            "8,Jake Bornheimer,196,90,Muhlenberg College,1927,New Brunswick,New Jersey\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df = spark.read.option('header', True).option('inferSchema', True).csv(\"Players.csv\")\n",
        "players_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlZ-HMhq4mpo",
        "outputId": "ad348488-ca21-42f3-87b1-0123da7590a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- Player: string (nullable = true)\n",
            " |-- height: integer (nullable = true)\n",
            " |-- weight: integer (nullable = true)\n",
            " |-- collage: string (nullable = true)\n",
            " |-- born: integer (nullable = true)\n",
            " |-- birth_city: string (nullable = true)\n",
            " |-- birth_state: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG8dzUTR4wZM",
        "outputId": "19ba7575-a940-498d-f939-2daa7fbc17ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "|_c0|          Player|height|weight|             collage|born|   birth_city| birth_state|\n",
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "|  0| Curly Armstrong|   180|    77|  Indiana University|1918|         NULL|        NULL|\n",
            "|  1|    Cliff Barker|   188|    83|University of Ken...|1921|     Yorktown|     Indiana|\n",
            "|  2|   Leo Barnhorst|   193|    86|University of Not...|1924|         NULL|        NULL|\n",
            "|  3|      Ed Bartels|   196|    88|North Carolina St...|1925|         NULL|        NULL|\n",
            "|  4|     Ralph Beard|   178|    79|University of Ken...|1927|  Hardinsburg|    Kentucky|\n",
            "|  5|      Gene Berce|   180|    79|Marquette University|1926|         NULL|        NULL|\n",
            "|  6|   Charlie Black|   196|    90|University of Kansas|1921|         Arco|       Idaho|\n",
            "|  7|     Nelson Bobb|   183|    77|   Temple University|1924| Philadelphia|Pennsylvania|\n",
            "|  8| Jake Bornheimer|   196|    90|  Muhlenberg College|1927|New Brunswick|  New Jersey|\n",
            "|  9|    Vince Boryla|   196|    95|University of Denver|1927| East Chicago|     Indiana|\n",
            "| 10|       Don Boven|   193|    95|Western Michigan ...|1925|    Kalamazoo|    Michigan|\n",
            "| 11|   Harry Boykoff|   208|   102|St. John's Univer...|1922|     Brooklyn|    New York|\n",
            "| 12|     Joe Bradley|   190|    79|Oklahoma State Un...|1928|   Washington|    Oklahoma|\n",
            "| 13|     Bob Brannum|   196|    97|Michigan State Un...|1925|         NULL|        NULL|\n",
            "| 14|      Carl Braun|   196|    81|  Colgate University|1927|     Brooklyn|    New York|\n",
            "| 15|   Frankie Brian|   185|    81|Louisiana State U...|1923|      Zachary|   Louisiana|\n",
            "| 16|Price Brookfield|   193|    83|West Texas A&M Un...|1920|     Floydada|       Texas|\n",
            "| 17|       Bob Brown|   193|    92|    Miami University|1923|   Versailles|        Ohio|\n",
            "| 18|      Jim Browne|   208|   106|                NULL|1930|   Midlothian|    Illinois|\n",
            "| 19|      Walt Budko|   196|    99| Columbia University|1925|      Kearney|  New Jersey|\n",
            "+---+----------------+------+------+--------------------+----+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1️⃣. Qual a média de altura e de peso dos jogadores?"
      ],
      "metadata": {
        "id": "OevbePGT44Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean, udf, round"
      ],
      "metadata": {
        "id": "NhP-59Sq5SMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.select(round(mean(col('height')/100), 2)).collect()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jTq8Djg5FcN",
        "outputId": "b7bfc18a-7f7a-419a-8471-5605663871c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.99"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solução alternativa\n",
        "\n",
        "from pyspark.sql.functions import mean, avg\n",
        "\n",
        "players_df.select( avg( players_df['height'] ),  mean( players_df.weight )  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcp8mBdA56Tq",
        "outputId": "6989f9ff-effe-44c8-cbb4-42be4a9009ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------+\n",
            "|       avg(height)|      avg(weight)|\n",
            "+------------------+-----------------+\n",
            "|198.70492221372098|94.78321856669217|\n",
            "+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players_df.agg( { 'height': 'avg', 'weight':'avg' } ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o0XSR-Q6V89",
        "outputId": "bba5e8d6-7062-4db8-f262-5afb0200d315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+------------------+\n",
            "|      avg(weight)|       avg(height)|\n",
            "+-----------------+------------------+\n",
            "|94.78321856669217|198.70492221372098|\n",
            "+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean, udf\n",
        "\n",
        "media_altura = df.select(round(mean(col('height')/100), 2)).collect()[0][0]\n",
        "media_peso = df.select(round(mean(col('weight')), 2)).collect()[0][0]\n",
        "\n",
        "print(\"\"\"Média de altura dos jogadores: {}m \\nMédia de peso dos jogadores: {}kg\n",
        "      \"\"\".format(media_altura, media_peso))"
      ],
      "metadata": {
        "id": "XLGSFi3I45Do"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}